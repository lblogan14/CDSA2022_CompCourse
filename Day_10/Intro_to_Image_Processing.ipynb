{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro_to_Image_Processing.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-E83WuIC90O"
      },
      "source": [
        "# Image Processing\n",
        "\n",
        "\n",
        "## Learning objectives\n",
        "\n",
        "* Understand how scalar data and color images are stored as multi-dimensional arrays\n",
        "* Access multi-dimensional data using multiple indices\n",
        "* Locate discrete objects using image segmentation; quantify their location, size, and shape\n",
        "* Load and use the external libraries `scikit-image` and `matplotlib` and use their documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading images from GitHub Directly"
      ],
      "metadata": {
        "id": "GOwrR1LbU8c2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS0x8tZnSlZs"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/lblogan14/CDSA2022_CompCourse/main/Day_10/300K-Nuclei-C8BTBT_crop.png'\n",
        "req = requests.get(url)\n",
        "Image.open(BytesIO(req.content))"
      ],
      "metadata": {
        "id": "u-H7gt9KTNqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZSkOGqsC90U"
      },
      "source": [
        "## Exercise 1: What is an image?\n",
        "\n",
        "You can think of a black & white picture as a 2D array of values, where each element of the array contains the brightness at that location.  \n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsYKbGHYC90S"
      },
      "outputs": [],
      "source": [
        "import skimage                  # for image-processing\n",
        "from skimage import io          # give ourselves a shortcut \n",
        "from skimage import measure    # give ourselves a shortcut "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pXrbZI3C90U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mysteryimage = np.zeros((8, 12))\n",
        "mysteryimage[::2, 1::2] = 80      # make a plaid pattern\n",
        "mysteryimage[1::2, ::2] = 30      # you can comment one or the other out to see what it does\n",
        "\n",
        "plt.imshow(mysteryimage, cmap='gray')   \n",
        "# the word cmap stands for 'colormap' \n",
        "# 'gray' specifies that you want to display a grayscale image\n",
        "print('Type = ', type(mysteryimage))\n",
        "print('Shape = ', np.shape(mysteryimage))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFgsDeVZC90V"
      },
      "source": [
        "When you run the code, an 8x12 image should appear. Check that you understand each of these details. Write your answers to each question by editing this cell.\n",
        "* *which set of indices goes with the rows and which with the colums?*\n",
        "* *where is the origin (0,0) located?*\n",
        "* *what is the data type of `mysteryimage`?*\n",
        "* *what command can you use to find the dimensions of the image?*\n",
        "\n",
        "**If you're interested:** take a look at how the `:` indexing works to create the pattern"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11tykbT_C90V"
      },
      "source": [
        "### Loading and interpreting monochrome images\n",
        "\n",
        "Here's how to read in an image into an array. Note that the code (as written) expects the file to be stored inside a folder named `pics` that is located in the same directory as this notebook. If that is not where you stored the image, you will need to edit that line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3plROU0C90W"
      },
      "outputs": [],
      "source": [
        "islands = io.imread(url) # <-- look what I put in the io.imread function there\n",
        "plt.imshow(islands)\n",
        "print('Type = ', type(islands))\n",
        "print('Shape = ', np.shape(islands))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3mQ2dePC90X"
      },
      "source": [
        "Notice that this image looks monochrome, but was (in fact) saved as a color image. Therefore, it is loaded as an $N \\times M \\times 3$ array. You can think of a color image as 3 arrays: one for the red channel, one for green channel, and one for the blue channel. Here, all three colors happen to be the same, so let's examine just one channel (I picked the red channel, but this was an arbitrary decision.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbJIeIBmC90X"
      },
      "outputs": [],
      "source": [
        "islands = islands[:,:,0]        # we're going to work with just the red (index=0) channel\n",
        "dim = np.shape(islands)\n",
        "plt.imshow(islands, cmap='gray')\n",
        "plt.plot([0, dim[1]-1],[60,60])   # draw a line connecting the two endpoints of the row at index=60\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks62u-fjC90Y"
      },
      "outputs": [],
      "source": [
        "plt.plot(islands[60,:])\n",
        "plt.xlabel('Pixel index (column)')\n",
        "plt.ylabel('Image intensity value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKgt5IXUC90Y"
      },
      "source": [
        "You can plot the data just along the blue row. Notice that there aren't any \"units\" associated with brightness. Most images are saved as integers (0 to 255) at each pixel.\n",
        "\n",
        "**To do:** In the cell below, copy both codes above and then modify them so that you plot a single COLUMN instead of a single row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYEZewr6C90Y"
      },
      "outputs": [],
      "source": [
        "# put your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x69WziaTC90Z"
      },
      "source": [
        "### Loading and interpreting color Images\n",
        "The Hubble Ultra Deep Field image is a color image, and we will use that image to examine how colors are saved inside an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2iYA_nPC90Z"
      },
      "outputs": [],
      "source": [
        "deepfield = io.imread('https://raw.githubusercontent.com/lblogan14/CDSA2022_CompCourse/main/Day_10/HubbleDeepField.jpg')\n",
        "plt.figure(figsize=(16,20))\n",
        "plt.imshow(deepfield)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63yEFw-vC90Z"
      },
      "outputs": [],
      "source": [
        "crop = deepfield[900:1000,400:550,:] # crop a small portion of the large image\n",
        "red = crop[:,:,0]                    # just the red channel: the #0 slice\n",
        "blue = crop[:,:,2]                   # just the blue channel: the #2 slice\n",
        "\n",
        "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,6))\n",
        "ax1.imshow(red, cmap='gray')\n",
        "ax1.set_title('Red Channel Only')\n",
        "ax2.imshow(blue, cmap='gray')\n",
        "ax2.set_title('Blue Channel Only')\n",
        "ax3.imshow(crop)\n",
        "ax3.set_title('Color Image')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzzDksGEC90a"
      },
      "source": [
        "Look at some of the colors of the astronomical objects. Based on the description at the top of the notebok, you should see redder-object and blue-objects. The redder objects in the Color Image show up primarily in the \"Red Channel\", while the bluer objects show up in the \"Blue Channel\"\n",
        "\n",
        "**To do:**  In the cell below, copy the code above and then modify it ALSO show the green (index=1) channel, and also to crop the image to a different sub-region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rFvjst0C90a"
      },
      "outputs": [],
      "source": [
        "# put your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg1gjmHaC90a"
      },
      "source": [
        "## Exercise 2: Image segmentation\n",
        "\n",
        "The process of locating discrete \"objects\" within an image is known as *image segmentation*, and there are a variety of algorithms used to complete this task. Here, we will use a very simple one: thresholding to find all regions above the mean brightness of the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4SnoAZiC90a"
      },
      "outputs": [],
      "source": [
        "# Step 1: Find all regions above the mean\n",
        "plt.imshow(islands, cmap='gray')\n",
        "plt.title('original image')\n",
        "plt.show()\n",
        "threshold = islands > islands.mean()\n",
        "# Ask yourself: what data type do you think the variable threshold is?\n",
        "# Add a line of code here to check your answer\n",
        "\n",
        "\n",
        "\n",
        "plt.imshow(threshold, cmap='gray')\n",
        "plt.title('thresholded image')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xS8ylUCTC90b"
      },
      "outputs": [],
      "source": [
        "# Step 2: label each connected-component with a unique name\n",
        "all_labels = measure.label(threshold)  \n",
        "# Each region of the image \"threshold\" is saved into\n",
        "# a new image named \"all_labels\" where the pixel has the \n",
        "# a value that is the label for that region\n",
        "\n",
        "# more info about this funcion here:\n",
        "# https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.label\n",
        "# which also says a little bit about how it works\n",
        "# you don't have to dig into the details: I just want you to know where they are\n",
        "\n",
        "# we can see how this works by looking at the data inside the image\n",
        "plt.imshow(all_labels,cmap='gray')\n",
        "# for example:\n",
        "row = 120\n",
        "col = 170\n",
        "plt.plot(col,row, 'rx')\n",
        "plt.show()\n",
        "print('x marks the region named', all_labels[row,col])\n",
        "# To do: move the row and col values around to see the different\n",
        "# name inside different regions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK6Av-SAC90b"
      },
      "source": [
        "**To do:**  Inside the code above, move the row and col values around to see the different \"name\" inside different regions. The red x should move as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF2HIfUYC90b"
      },
      "source": [
        "## Exercise 3: Region properties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T7R4qFhC90c"
      },
      "source": [
        "We will use code from this library: https://scikit-image.org/docs/dev/api/skimage.measure.html\n",
        "\n",
        "The \"Notes\" section under this heading is particuarly useful for learning about the various properties you can measure: https://scikit-image.org/docs/dev/api/skimage.measure.html#skimage.measure.regionprop\n",
        "\n",
        "**To do:** Your task is to learn how these libraries work by using some examples given below. With your group, find out what each function is for. \n",
        "\n",
        "* **Option 1:** read the documentation for each of the functions we use below\n",
        "* **Option 2:** see if you can figure out from the name of the function and its output what the function does, then you can check your answer against the documentation\n",
        "\n",
        "When you have it figured out, add an informative label and units (if these exist) to each print statement below, in place of the useless text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frmJAvbCC90c"
      },
      "outputs": [],
      "source": [
        "# Step 3 Calculate image properties \n",
        "\n",
        "regions = measure.regionprops(all_labels)\n",
        "\n",
        "print('Edit this line: ', len(regions))\n",
        "\n",
        "print('Edit this line:', regions[52].centroid, '[pixels]')\n",
        "\n",
        "rowscols = regions[5].coords\n",
        "print('Edit this line:', rowscols[7,0], rowscols[7,1], islands[rowscols[7,0],rowscols[7,1]])\n",
        "\n",
        "areas = [prop.area for prop in regions]\n",
        "print('Edit this line:', areas[24])\n",
        "count = 0\n",
        "for i in range(len(regions)):\n",
        "    if areas[i] > 10:\n",
        "        count += 1\n",
        "print('Edit this line:', count)\n",
        "\n",
        "ecc = [prop.eccentricity for prop in regions]\n",
        "print('Edit this line:', ecc[24])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKYccmONC90c"
      },
      "source": [
        "## Exercise 4: Plot a histogram of areas\n",
        "\n",
        "A histogram is a plot that lets you examine a population of measurements and see how the values are distributed. Characterizing this is one of the measurements the researchers studying this image need to do.\n",
        "\n",
        "Below is an example of how to plot a histogram. Edit the code so that it plots the area data from Exercise 3 (rather than `randomdata`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQb8qDEyC90c"
      },
      "outputs": [],
      "source": [
        "randomdata = 5*np.random.randn(1000)    # this is a way to generate some random data\n",
        "                                        # you'll want to comment out this line to do this exercise\n",
        "\n",
        "    \n",
        "#plot a histogram of your data (not the randomdata!)    \n",
        "plt.hist(randomdata, bins=20)\n",
        "\n",
        "#if you want more control over your bins, you can play with these \n",
        "#mybins = np.linspace(-20,20,21)\n",
        "#plt.hist(randomdata, bins=mybins)\n",
        "\n",
        "plt.xlabel('value [units?]')\n",
        "plt.ylabel('# of occurances')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenCV\n",
        "What computer vision scientists actually use....\n",
        "https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html"
      ],
      "metadata": {
        "id": "97sNFFn5ZOS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import urllib.request"
      ],
      "metadata": {
        "id": "jqLk5yCHZe-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_scene = 'https://raw.githubusercontent.com/lblogan14/CDSA2022_CompCourse/main/Day_10/input.jpg'\n",
        "resp = urllib.request.urlopen(url_scene)\n",
        "img = np.array(bytearray(resp.read()), dtype='uint8')\n",
        "img = cv2.imdecode(img, cv2.IMREAD_COLOR)"
      ],
      "metadata": {
        "id": "GabOzkNvaBCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# something is not right...\n",
        "#img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "XjdnQZvVTSQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "plt.imshow(gray_img, cmap='gray')"
      ],
      "metadata": {
        "id": "PrTt2aksZ2kP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image color spaces\n",
        "* **RBG**: Red, Green, Blue. Every pixel value is represented as a tuple of three numbers corresponding to red, green, and blue. Each value ranges between 0 and 255\n",
        "* **YUV**: Y refers to the luminance or intensity, U/V channels represent color information.\n",
        "* **HSV**: Hue, Satruation, Value"
      ],
      "metadata": {
        "id": "3Tpdrevic0hW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert color spaces\n",
        "To see a list of all available flags of color spaces"
      ],
      "metadata": {
        "id": "7wpqq5pTdCeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([x for x in dir(cv2) if x.startswith('COLOR_')])"
      ],
      "metadata": {
        "id": "dtZDmJnidCHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can convert color space using cv2.cvtColor\n",
        "gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "plt.imshow(gray_img, cmap='gray')"
      ],
      "metadata": {
        "id": "XNbG3adwcVQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split image channels\n",
        "To convert color space to YUV"
      ],
      "metadata": {
        "id": "98naz5RidjYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yuv_img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
        "plt.imshow(yuv_img)"
      ],
      "metadata": {
        "id": "VgIAjbu0dedJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It may look weird at first.... Separate each channel to see the details:"
      ],
      "metadata": {
        "id": "MAj-9wupdvIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# alternative 1\n",
        "y,u,v = cv2.split(yuv_img)\n",
        "plt.figure(figsize=(15,12))\n",
        "plt.subplot(311)\n",
        "plt.imshow(y)\n",
        "plt.title('Y Channel')\n",
        "plt.subplot(312)\n",
        "plt.imshow(u)\n",
        "plt.title('U Channel')\n",
        "plt.subplot(313)\n",
        "plt.imshow(v)\n",
        "plt.title('V Channel')"
      ],
      "metadata": {
        "id": "ZXeZqAj0dtDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# alternative 2 (faster)\n",
        "plt.figure(figsize=(15,12))\n",
        "plt.subplot(311)\n",
        "plt.imshow(yuv_img[:,:,0], label='Y Channel')\n",
        "plt.subplot(312)\n",
        "plt.imshow(yuv_img[:,:,1], label='U Channel')\n",
        "plt.subplot(313)\n",
        "plt.imshow(yuv_img[:,:,2], label='V Channel')"
      ],
      "metadata": {
        "id": "yy4i1Gbud2Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge image channels\n",
        "read an image, split it into separate channels, and merge them to see how different effects can be obtained out of different combinations"
      ],
      "metadata": {
        "id": "kR-5tfJNeAMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r,g,b = cv2.split(img)\n",
        "gbr_img = cv2.merge((g,b,r))\n",
        "rbr_img = cv2.merge((r,b,r))\n",
        "rgb_img = cv2.merge((r,g,b))\n",
        "\n",
        "plt.figure(figsize=(15,12))\n",
        "plt.subplot(311)\n",
        "plt.imshow(rgb_img)\n",
        "plt.title('Original Image')\n",
        "plt.subplot(312)\n",
        "plt.imshow(gbr_img)\n",
        "plt.title('GBR Image')\n",
        "plt.subplot(313)\n",
        "plt.imshow(rbr_img)\n",
        "plt.title('RBR Image')"
      ],
      "metadata": {
        "id": "t9JEYu_9d6kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image translation"
      ],
      "metadata": {
        "id": "boehDu8reuXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "shifting an image"
      ],
      "metadata": {
        "id": "1e3AcdKQewMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = rgb_img\n",
        "num_rows, num_cols = img.shape[:2]\n",
        "translation_matrix = np.float32([ [1,0,70], [0,1,110]])\n",
        "img_translation = cv2.warpAffine(img, translation_matrix, (num_cols, num_rows), cv2.INTER_LINEAR)\n",
        "plt.imshow(img_translation)\n",
        "plt.title('Translation')"
      ],
      "metadata": {
        "id": "h6alNFYheJZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translation means to shift the image by adding/substracting the *x* and *y* coordinates. To do this, need to create a transformation matrix defined as follows,\n",
        "$$T = \\begin{bmatrix} 1 & 0 & t_x \\\\ 0 & 1& t_y \\end{bmatrix}$$\n",
        "The $t_x$ and $t_y$ values are the *x* and *y* translation values. The image will be moved by *x* units to the right and by *y* units downwards. Then we use the `warpAffine` function to translate the image. The third argument in `warpAffine` refers to the number of rows and columns in the resulting image. It passes `InterpolaitonFlags` which defines combination of interpolation methods. In this case, it is `INTER_LINEAR` method.\n",
        "\n",
        "Snce the number of rows and columns is the same as the original image, the resultant\n",
        "image is going to get cropped. The reason for this is that the display space is not enough in the\n",
        "output when we applied the translation matrix. To avoid cropping,"
      ],
      "metadata": {
        "id": "IaawqT1xe6gQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_translation = cv2.warpAffine(img, translation_matrix, (num_cols+70, num_rows+110))\n",
        "plt.imshow(img_translation)\n",
        "plt.title('Translation without chopping')"
      ],
      "metadata": {
        "id": "YfBlbW0Feyv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To move the image to the middle of a bigger image frame,"
      ],
      "metadata": {
        "id": "ZkLv5eULfBaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translation_matrix = np.float32([[1,0,70], [0,1,110]])\n",
        "img_translation = cv2.warpAffine(img, \n",
        "                                 translation_matrix, \n",
        "                                 (num_cols+70, num_rows+110)\n",
        "                                )\n",
        "translation_matrix = np.float32([[1,0,-30], [0,1,-50]])\n",
        "img_translation = cv2.warpAffine(img_translation, \n",
        "                                 translation_matrix, \n",
        "                                 (num_cols+70+30, num_rows+110+50))\n",
        "plt.imshow(img_translation)"
      ],
      "metadata": {
        "id": "KQ2uy1A5e9NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`borderMode` and `borderValue` allow you to fill up the empty borders of the translation with a pixel extrapolation method,"
      ],
      "metadata": {
        "id": "N76WVm0afE_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translation_matrix = np.float32([[1,0,70], [0,1,110]])\n",
        "img_translation = cv2.warpAffine(img, \n",
        "                                 translation_matrix, \n",
        "                                 (num_cols+70, num_rows+110),\n",
        "                                 cv2.INTER_LINEAR,\n",
        "                                 cv2.BORDER_WRAP, # <- borderMode\n",
        "                                 1) # <- borderValue\n",
        "plt.imshow(img_translation)"
      ],
      "metadata": {
        "id": "bCW1k8MAfCxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image rotation"
      ],
      "metadata": {
        "id": "EiWY4dv0fILh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rotation_matrix = cv2.getRotationMatrix2D((num_cols/2, num_rows/2),\n",
        "                                          30,\n",
        "                                          0.7)\n",
        "img_rotation = cv2.warpAffine(img, rotation_matrix, (num_cols, num_rows))\n",
        "plt.imshow(img_rotation)"
      ],
      "metadata": {
        "id": "XjedJtuOfGHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`getRotationMatrix2D` specify the center point around which the image would be rotated as the first argument, then the angl of rotation in degrees, and a scaling factor for the image at the end. `30` as the second argument is to rotated the image by 30 degrees and `0.7` is to shrink the image by 30%"
      ],
      "metadata": {
        "id": "mI5GwEfrfLsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rotation_matrix"
      ],
      "metadata": {
        "id": "ynhLz22HfKbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image scaling"
      ],
      "metadata": {
        "id": "IT7_5XLwfirX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resize an image"
      ],
      "metadata": {
        "id": "dydei1QpfkZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_scaled = cv2.resize(img, None, fx=1.5, fy=1.5, interpolation=cv2.INTER_LINEAR)\n",
        "plt.imshow(img_scaled)\n",
        "plt.title('Scaling - Linear interpolation')"
      ],
      "metadata": {
        "id": "Zbu90mD-fSZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_scaled = cv2.resize(img, None, fx=1.5, fy=1.5, interpolation=cv2.INTER_CUBIC)\n",
        "plt.imshow(img_scaled)\n",
        "plt.title('Scaling - Cubic Interpolation')"
      ],
      "metadata": {
        "id": "HHpwRCiPfmyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `fx` and `fy` are the scaling factors. In this case, the image will be enlarged by a factor of 1.5"
      ],
      "metadata": {
        "id": "AOTGtsXHfx4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_scaled = cv2.resize(img, (450, 400), interpolation=cv2.INTER_AREA)\n",
        "plt.imshow(img_scaled)\n",
        "plt.title('Scaling - Skewed Size')"
      ],
      "metadata": {
        "id": "IHTKwXhefvJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the `None` arg is replaced by a specific size, in this case, `(450, 400)`, the `resize` function will use that size to skew the image and resize it to that size."
      ],
      "metadata": {
        "id": "9KqcL203f3FF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Affine transformations\n",
        "**Euclidean transformations** are a type of geometric transformation that preserve length and\n",
        "angle measures.\n",
        " If we take a geometric shape and apply Euclidean transformation to it, the\n",
        "shape will remain unchanged. It might look rotated, shifted, and so on, but the basic\n",
        "structure will not change.\n",
        "\n",
        "**Affine transformations** are generalizations of Euclidean transformations.  Under the realm of affine transformations, lines will remain lines, but squares might become rectangles or parallelograms. Basically, affine\n",
        "transformations don't preserve lengths and angles.\n",
        "\n",
        "In order to build a general affine transformation matrix, we need to define the control\n",
        "points. Once we have these control points, we need to decide where we want them to be\n",
        "mapped.\n",
        "\n",
        " In this particular situation, all we need are three points in the source image, and\n",
        "three points in the output image. If we want to convert an image into a parallelogram-like image,"
      ],
      "metadata": {
        "id": "pANaQiNLf4s-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_points = np.float32([[0,0], \n",
        "                         [num_cols-1,0], \n",
        "                         [0,num_rows-1]])\n",
        "dst_points = np.float32([[0,0], \n",
        "                         [int(0.6*(num_cols-1)),0], \n",
        "                         [int(0.4*(num_cols-1)),num_rows-1]])\n",
        "\n",
        "affine_matrix = cv2.getAffineTransform(src_points, dst_points)\n",
        "img_output = cv2.warpAffine(img, affine_matrix, (num_cols, num_rows))\n",
        "plt.figure(figsize=(15,12))\n",
        "plt.subplot(211)\n",
        "plt.imshow(img)\n",
        "plt.title('Input')\n",
        "plt.subplot(212)\n",
        "plt.imshow(img_output)\n",
        "plt.title('Output')"
      ],
      "metadata": {
        "id": "d2wYOzKuf0dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Projective transformations\n",
        "Any two images on a given plane are related by a homography. Once the camera rotation and translation have been extracted from an estimated\n",
        "homography matrix, this information may be used for navigation, or to insert models of 3D\n",
        "objects into an image or video"
      ],
      "metadata": {
        "id": "fp1nAxYHgNR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_points = np.float32([[0,0], \n",
        "                         [num_cols-1,0], \n",
        "                         [0,num_rows-1], \n",
        "                         [num_cols-1,num_rows-1]])\n",
        "dst_points = np.float32([[0,0], [num_cols-1,0], \n",
        "                         [int(0.33*num_cols),num_rows-1],\n",
        "                         [int(0.66*num_cols),num_rows-1]])\n",
        "\n",
        "projective_matrix = cv2.getPerspectiveTransform(src_points, dst_points)\n",
        "img_output = cv2.warpPerspective(img, projective_matrix, (num_cols, num_rows))\n",
        "plt.figure(figsize=(15,12))\n",
        "plt.subplot(211)\n",
        "plt.imshow(img)\n",
        "plt.title('Input')\n",
        "plt.subplot(212)\n",
        "plt.imshow(img_output)\n",
        "plt.title('Output')"
      ],
      "metadata": {
        "id": "rHE40zLLgCJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose four control points in the source image and map them to the destination\n",
        "image. Parallel lines will not remain parallel lines after the transformation. The `getPerspectiveTransform()` function is used to get the transformation matrix."
      ],
      "metadata": {
        "id": "YSxo8obggSbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_points = np.float32([[0,0], \n",
        "                         [0,num_rows-1], \n",
        "                         [num_cols/2,0],\n",
        "                         [num_cols/2,num_rows-1]])\n",
        "dst_points = np.float32([[0,100], \n",
        "                         [0,num_rows-101],\n",
        "                         [num_cols/2,0],\n",
        "                         [num_cols/2,num_rows-1]])\n",
        "projective_matrix = cv2.getPerspectiveTransform(src_points, dst_points)\n",
        "img_output = cv2.warpPerspective(img, projective_matrix, (num_cols, num_rows))\n",
        "plt.figure(figsize=(15,12))\n",
        "plt.subplot(211)\n",
        "plt.imshow(img)\n",
        "plt.title('Input')\n",
        "plt.subplot(212)\n",
        "plt.imshow(img_output)\n",
        "plt.title('Output')"
      ],
      "metadata": {
        "id": "o-xME_r_gRYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image warping\n",
        "Create a custom mapping"
      ],
      "metadata": {
        "id": "6x6kIKpAgXEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img.shape"
      ],
      "metadata": {
        "id": "hBP2WSangTod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gray_img.shape"
      ],
      "metadata": {
        "id": "lCNkE9MxgZT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows, cols = gray_img.shape"
      ],
      "metadata": {
        "id": "LFl6O3qkgjlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vertical wave\n",
        "img_output = np.zeros(gray_img.shape, dtype=gray_img.dtype)\n",
        "\n",
        "for i in range(rows):\n",
        "  for j in range(cols):\n",
        "    offset_x = int(25.0 * np.sin(2*3.14*i/180))\n",
        "    offset_y = 0\n",
        "    if j+offset_x < rows:\n",
        "      img_output[i,j] = gray_img[i,(j+offset_x)%cols]\n",
        "    else:\n",
        "      img_output[i,j] = 0\n",
        "      \n",
        "plt.figure(figsize=(15,12))\n",
        "plt.subplot(211)\n",
        "plt.imshow(gray_img, cmap='gray')\n",
        "plt.title('Input')\n",
        "plt.subplot(212)\n",
        "plt.imshow(img_output, cmap='gray')\n",
        "plt.title('Output')"
      ],
      "metadata": {
        "id": "QfwEWpCogmIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# horizontal wave\n",
        "img_output = np.zeros(gray_img.shape, dtype=gray_img.dtype)\n",
        "\n",
        "for i in range(rows):\n",
        "  for j in range(cols):\n",
        "    offset_x = 0\n",
        "    offset_y = int(16.0*np.sin(2*3.14*j/150))\n",
        "    if i+offset_y < cols:\n",
        "      img_output[i,j] = gray_img[(i+offset_y)%rows, j]\n",
        "    else:\n",
        "      img_output[i,j] = 0\n",
        "      \n",
        "plt.figure(figsize=(15,12))\n",
        "plt.subplot(211)\n",
        "plt.imshow(gray_img, cmap='gray')\n",
        "plt.title('Input')\n",
        "plt.subplot(212)\n",
        "plt.imshow(img_output, cmap='gray')\n",
        "plt.title('Output')"
      ],
      "metadata": {
        "id": "qSULbpNtgtAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# both horizontal and vertical waves\n",
        "img_output = np.zeros(gray_img.shape, dtype=gray_img.dtype) \n",
        " \n",
        "for i in range(rows): \n",
        "    for j in range(cols): \n",
        "        offset_x = int(20.0 * np.sin(2 * 3.14 * i / 150)) \n",
        "        offset_y = int(20.0 * np.cos(2 * 3.14 * j / 150)) \n",
        "        if i+offset_y < rows and j+offset_x < cols: \n",
        "            img_output[i,j] = gray_img[(i+offset_y)%rows,(j+offset_x)%cols] \n",
        "        else: \n",
        "            img_output[i,j] = 0 \n",
        "            \n",
        "plt.figure(figsize=(15,12))\n",
        "plt.subplot(211)\n",
        "plt.imshow(gray_img, cmap='gray')\n",
        "plt.title('Input')\n",
        "plt.subplot(212)\n",
        "plt.imshow(img_output, cmap='gray')\n",
        "plt.title('Output')"
      ],
      "metadata": {
        "id": "Yo9PweLsgzJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Blurring\n",
        "known as **low pass filter**\n",
        "A low pass filter is a filter that allows low frequencies, and blocks higher\n",
        "frequencies. The frequency in an image refers to the rate of change of pixel values. So the sharp edges would be high-frequency content because the pixel values change rapidly in that region, and the plain areas would be low-frequency\n",
        "content. Going by this definition, a low pass filter would try to smooth the edges.\n",
        "\n",
        "A simple way to build a low pass filter is by uniformly averaging the values in the\n",
        "neighborhood of a pixel."
      ],
      "metadata": {
        "id": "YunBMFhVhjtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_identity = np.array([[0,0,0], [0,1,0], [0,0,0]])\n",
        "kernel_4x4 = np.ones((4,4), np.float32) / 16.0\n",
        "kernel_6x6 = np.ones((6,6), np.float32) / 36.0"
      ],
      "metadata": {
        "id": "uFkmkexphjUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(img)\n",
        "plt.title('Original')"
      ],
      "metadata": {
        "id": "SEBDvBupg5d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = cv2.filter2D(img, -1, kernel_identity)\n",
        "# value -1 is to maintain source image depth\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(output)\n",
        "plt.title('Identity Filter')"
      ],
      "metadata": {
        "id": "LuOwkbiuiadw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = cv2.filter2D(img, -1, kernel_4x4)\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(output)\n",
        "plt.title('4x4 Filter')"
      ],
      "metadata": {
        "id": "ty9PIPSFieCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = cv2.filter2D(img, -1, kernel_6x6)\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(output)\n",
        "plt.title('6x6 Filter')"
      ],
      "metadata": {
        "id": "PYq1k1deiiA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Size of the kernel versus blurriness\n",
        "In the previous examples, the `filter2D` function is applied to the input image. As the kernel size increases, the images get blurrier.\n",
        "\n",
        "Another way to do this is to apply the `blur` function if you don't want to generate the kernels by yourself."
      ],
      "metadata": {
        "id": "lDDG5xTPiq_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = cv2.blur(img, (6,6))\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(output)\n",
        "plt.title('6x6 Filter')"
      ],
      "metadata": {
        "id": "sFZIiWWTilxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Motion blur\n",
        "When we apply the motion blurring effect, it will look like you captured the picture while\n",
        "moving in a particular direction"
      ],
      "metadata": {
        "id": "59ZJP-wEiv3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "size = 15\n",
        "\n",
        "# generate kernel\n",
        "kernel_motion_blur = np.zeros((size, size))\n",
        "kernel_motion_blur[int((size-1)/2), :] = np.ones(size)\n",
        "kernel_motion_blur = kernel_motion_blur / size\n",
        "\n",
        "# apply kernel to input image\n",
        "output = cv2.filter2D(img, -1, kernel_motion_blur)\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(img)\n",
        "plt.title('Original')\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(output)\n",
        "plt.title('Motion Blur')"
      ],
      "metadata": {
        "id": "7AWX15switoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The motion blur kernel averages the pixel values in a particular direction. It's like a directional low pass filter. \n",
        "\n",
        "For example, a 3x3 horizontal motion-blurring kernel will look like this,\n",
        "$$M=\\begin{bmatrix} 0 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix}$$\n",
        "This will blur the image in a horizontal direction.\n",
        "\n",
        "The example shown above applies a 15x15 kernel."
      ],
      "metadata": {
        "id": "lzNK1r8yi68S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sharpening\n",
        "Applying the sharpening filter will sharpen the edges in the image. This filter is very useful\n",
        "when we want to enhance the edges of an image that's not crisp enough"
      ],
      "metadata": {
        "id": "VMzNuHCWi-LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate kernels\n",
        "kernel_sharpen_1 = np.array([[-1,-1,-1], \n",
        "                             [-1,9,-1], \n",
        "                             [-1,-1,-1]])\n",
        "kernel_sharpen_2 = np.array([[1,1,1],\n",
        "                             [1,-7,1],\n",
        "                             [1,1,1]])\n",
        "kernel_sharpen_3 = np.array([[-1,-1,-1,-1,-1],\n",
        "                             [-1,2,2,2,-1], \n",
        "                             [-1,2,8,2,-1], \n",
        "                             [-1,2,2,2,-1], \n",
        "                             [-1,-1,-1,-1,-1]]) / 8.0 \n",
        "\n",
        "# apply kernels to input image\n",
        "output_1 = cv2.filter2D(img, -1, kernel_sharpen_1)\n",
        "output_2 = cv2.filter2D(img, -1, kernel_sharpen_2)\n",
        "output_3 = cv2.filter2D(img, -1, kernel_sharpen_3)"
      ],
      "metadata": {
        "id": "u8XcoOR6i12j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To just sharpen the image, we can apply a kernel like this,\n",
        "$$M = \\begin{bmatrix} -1 & -1 & -1 \\\\ -1 & 9 & -1 \\\\ -1 & -1 & -1 \\end{bmatrix}$$\n",
        "and the result is shown below,"
      ],
      "metadata": {
        "id": "0n3kxNt1jEgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(output_1)\n",
        "plt.title('Sharpening')"
      ],
      "metadata": {
        "id": "cfCJuxysjDKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To just excessively sharpen the image, we can apply a kernel like this,\n",
        "$$M = \\begin{bmatrix} 1 & 1 & 1 \\\\ 1 & -7 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix}$$\n",
        "and the result is shown below,"
      ],
      "metadata": {
        "id": "JJTrAgMcjKeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(output_2)\n",
        "plt.title('Excessive Sharpening')"
      ],
      "metadata": {
        "id": "nWe6VhVbjH8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want our images to look more natural, we would use an edge enhancement filter. The\n",
        "underlying concept remains the same, but we use an approximate Gaussian kernel to build\n",
        "this filter. It will help us smooth the image when we enhance the edges, thus making the\n",
        "image look more natural.\n",
        "\n",
        "To achievethis, we can apply a kernel like this,\n",
        "$$M = \\begin{bmatrix} -1 & -1 & -1 & -1 & -1\\\\ -1 & 2 & 2 & 2 & -1 \\\\ -1 & 2 & 8 & 2 & -1 \\\\  -1 & 2 & 2 & 2 & -1 \\\\ -1 & -1 & -1 & -1 & -1\\end{bmatrix}$$\n",
        "and the result is shown below,"
      ],
      "metadata": {
        "id": "Cg_b9NhajO5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(output_3)\n",
        "plt.title('Edge Enhancement')"
      ],
      "metadata": {
        "id": "PeaTyFu7jMq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embossing\n",
        "An embossing filter will take an image and convert it to an embossed image. Each pixel will be replaced with a shadow or a highlight."
      ],
      "metadata": {
        "id": "x_MzVhrJjUXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate kernels\n",
        "kernel_emboss_1 = np.array([[0,-1,-1],\n",
        "                            [1,0,-1],\n",
        "                            [1,1,0]])\n",
        "kernel_emboss_2 = np.array([[-1,-1,0],\n",
        "                            [-1,0,1],\n",
        "                            [0,1,1]])\n",
        "kernel_emboss_3 = np.array([[1,0,0],\n",
        "                            [0,0,0],\n",
        "                            [0,0,-1]])\n",
        "\n",
        "# convert input image to grayscale\n",
        "gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "# apply kernels to grayscale image and add offset to produce the shadow\n",
        "output_1 = cv2.filter2D(gray_img, -1, kernel_emboss_1) + 128\n",
        "output_2 = cv2.filter2D(gray_img, -1, kernel_emboss_2) + 128\n",
        "output_3 = cv2.filter2D(gray_img, -1, kernel_emboss_3) + 128"
      ],
      "metadata": {
        "id": "4yEQdWpYja_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(img)\n",
        "plt.title('Original')\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(output_1, cmap='gray')\n",
        "plt.title('Embossing - South West')\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(output_2, cmap='gray')\n",
        "plt.title('Embossing - South East')\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(output_3, cmap='gray')\n",
        "plt.title('Embossing - North West')"
      ],
      "metadata": {
        "id": "9vzD-Hn3jRMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The embossing effect is\n",
        "achieved by offsetting all the pixel values in the image by `128`. This operation adds the\n",
        "highlight/shadow effect to the picture."
      ],
      "metadata": {
        "id": "aN0dWWKpj1r-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Edge detection\n",
        "The process of edge detection involves detecting sharp edges in the image, and producing a\n",
        "binary image as the output. Edge detection can be thought of as a high pass filtering operation.  A\n",
        "high pass filter allows high-frequency content to pass through and blocks the low-frequency\n",
        "content. As we discussed earlier, edges are high-frequency content. In edge detection, we\n",
        "want to retain these edges and discard everything else. \n",
        "\n",
        "`Sobel` filter is composed of,\n",
        "$$S_x=\\begin{bmatrix} -1 & 0 & 1 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1 \\end{bmatrix} \\quad\\quad\\quad \n",
        "S_y =\\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 &0 \\\\ 1 & 2 & 1 \\end{bmatrix}$$\n",
        "$S_x$ detects horizontal edges and $S_y$ detects vertical edges"
      ],
      "metadata": {
        "id": "ArufXNa_j3Al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_geo = 'https://raw.githubusercontent.com/lblogan14/CDSA2022_CompCourse/main/Day_10/geometrics_input.png'\n",
        "resp = urllib.request.urlopen(url_geo)\n",
        "img_geo = np.array(bytearray(resp.read()), dtype='uint8')\n",
        "img_geo = cv2.imdecode(img_geo, cv2.IMREAD_GRAYSCALE)"
      ],
      "metadata": {
        "id": "TD3grWwkkq7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it is used depth of cv2.CV_64F\n",
        "sobel_horizontal = cv2.Sobel(img_geo, cv2.CV_64F, 1, 0, ksize=5)\n",
        "# kernel size can be: 1, 3, 5, 7\n",
        "sobel_vertical = cv2.Sobel(img_geo, cv2.CV_64F, 0, 1, ksize=5)"
      ],
      "metadata": {
        "id": "a-TRq0-vjkT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case of 8-bit input images, it will result in truncated derivatives, so depth\n",
        "value `cv2.CV_16U` can be used instead. In case edges are not that well-defined the value\n",
        "of kernel can be adjusted, minor to obtain thinner edges and major for the opposite purpose. "
      ],
      "metadata": {
        "id": "U6Y9MEiMkDC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(img_geo, cmap='gray')\n",
        "plt.title('Original')\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(sobel_horizontal, cmap='gray')\n",
        "plt.title('Sobel horizontal')\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(sobel_vertical, cmap='gray')\n",
        "plt.title('Sobel vertical')\n",
        "laplacian = cv2.Laplacian(img_geo, cv2.CV_64F)\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(laplacian, cmap='gray')\n",
        "plt.title('Laplacian')"
      ],
      "metadata": {
        "id": "IhfFDdhEkFd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Laplacian` does not work well given too much noise in an image. In this case, try `Canny` edge detector,"
      ],
      "metadata": {
        "id": "LgkkM6gele1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_train = 'https://raw.githubusercontent.com/lblogan14/CDSA2022_CompCourse/main/Day_10/train_input.png'\n",
        "resp = urllib.request.urlopen(url_train)\n",
        "img_train = np.array(bytearray(resp.read()), dtype='uint8')\n",
        "img_train = cv2.imdecode(img_train, cv2.IMREAD_GRAYSCALE)"
      ],
      "metadata": {
        "id": "xyM6IztXkKmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "laplacian = cv2.Laplacian(img_train, cv2.CV_64F)\n",
        "canny = cv2.Canny(img_train, 50, 240)"
      ],
      "metadata": {
        "id": "O4bjUQ18mYQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(img_train, cmap='gray')\n",
        "plt.title('Original')\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(laplacian, cmap='gray')\n",
        "plt.title('Laplacian')\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(canny, cmap='gray')\n",
        "plt.title('Canny')"
      ],
      "metadata": {
        "id": "bbYXoiYpmb9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Laplacian kernel gives rise to a noisy output, the edges are not specified much clearly shown in the image."
      ],
      "metadata": {
        "id": "UmKOz6p1mmA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `Canny` edge detector is much better. \n",
        "\n",
        "`Canny` takes two numbers as arguments to indicate the thresholds. The second argument is the **low threshold\n",
        "value**, and the third argument is the **high threshold value**. If the gradient value is\n",
        "beyond the high threshold value, it is marked as a strong edge. The `Canny` edge detector\n",
        "starts tracking the edge from this point and continues the process until the gradient value\n",
        "falls below the low threshold value. As you increase these thresholds, the weaker edges will\n",
        "be ignored. The output image will be cleaner and sparser. "
      ],
      "metadata": {
        "id": "QRc49iJcmodU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a vignette filter"
      ],
      "metadata": {
        "id": "p8XcUQFhmqIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_flr = 'https://raw.githubusercontent.com/lblogan14/CDSA2022_CompCourse/main/Day_10/flower_input.png'\n",
        "resp = urllib.request.urlopen(url_flr)\n",
        "img_flr = np.array(bytearray(resp.read()), dtype='uint8')\n",
        "img_flr = cv2.imdecode(img_flr, cv2.IMREAD_COLOR)\n",
        "img_flr = cv2.cvtColor(img_flr, cv2.COLOR_BGR2RGB)\n",
        "rows, cols = img_flr.shape[:2]"
      ],
      "metadata": {
        "id": "KRbt8t8bmgz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate vignette mask using Gaussian kernels\n",
        "kernel_x = cv2.getGaussianKernel(cols, 200)\n",
        "kernel_y = cv2.getGaussianKernel(rows, 200)\n",
        "kernel = kernel_y * kernel_x.T\n",
        "mask = 255 * kernel / np.linalg.norm(kernel)\n",
        "output = np.copy(img_flr)\n",
        "\n",
        "# apply mask to each channel in the input image\n",
        "for i in range(3):\n",
        "  output[:,:,i] = output[:,:,i] * mask"
      ],
      "metadata": {
        "id": "xVPhhm6Om8Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(img_flr)\n",
        "plt.title('Original')\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(output)\n",
        "plt.title('Vignette')"
      ],
      "metadata": {
        "id": "RoUxKaeznAfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vignette filter basically focuses the brightness on a particular part of the image and the\n",
        "other parts look faded. In order to achieve this, we need to filter out each channel in the\n",
        "image using a Gaussian kernel, by `getGaussianKernel` function.\n",
        "\n",
        "The second parameter of `getGaussianKernel` is the standard deviation of the Gaussian and controls the radius of the bright central region.\n",
        "\n",
        "Once the 2D kernel is built, a mask is built by normalizing this kernel and scaling it up: \\\\\n",
        "`mask = 255 * kernel/np.linalg.norm(kernel)` \\\\\n",
        "This is an important step because if you don't scale it up, the image will look black. This\n",
        "happens because all the pixel values will be close to zero after you superimpose the mask\n",
        "on the input image."
      ],
      "metadata": {
        "id": "HZ5Q5aVHnPLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To focus on a different region in the image,"
      ],
      "metadata": {
        "id": "HLynCmQRne_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generating vignette mask using Gaussian kernels \n",
        "kernel_x = cv2.getGaussianKernel(int(1.5*cols),200) \n",
        "kernel_y = cv2.getGaussianKernel(int(1.5*rows),200) \n",
        "kernel = kernel_y * kernel_x.T \n",
        "mask = 255 * kernel / np.linalg.norm(kernel) \n",
        "mask = mask[int(0.5*rows):, int(0.5*cols):] \n",
        "output = np.copy(img_flr) \n",
        " \n",
        "# applying the mask to each channel in the input image \n",
        "for i in range(3): \n",
        "  output[:,:,i] = output[:,:,i] * mask "
      ],
      "metadata": {
        "id": "npq1kJywnc7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All we need to do is build a bigger Gaussian kernel, and make sure that the peak coincides\n",
        "with the region of interest."
      ],
      "metadata": {
        "id": "pfZVShUWnRug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(img_flr)\n",
        "plt.title('Original')\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(output)\n",
        "plt.title('Vignette')"
      ],
      "metadata": {
        "id": "2jzeUaDInKow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enhance the contrast in an image\n",
        "The pixel values tend to concentrate near zero when we capture the images in a low-light condition. When this happens, a lot of details in the image are not clearly visible to the human eye. Use the **histogram equalization** to enhance the contrast to capture the details."
      ],
      "metadata": {
        "id": "Q-IjmQSgns2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(gray_img, cmap='gray')\n",
        "plt.title('Input')"
      ],
      "metadata": {
        "id": "nOCHduDMno5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.hist(gray_img.flatten())\n",
        "plt.title('Histogram of Input')"
      ],
      "metadata": {
        "id": "iesKYshtnx-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# equalize the histogram of input image\n",
        "histeq = cv2.equalizeHist(gray_img)"
      ],
      "metadata": {
        "id": "bGj-CR5coF8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.hist(histeq.flatten())\n",
        "plt.title('Histogram of Hist-Equalized Input')"
      ],
      "metadata": {
        "id": "SgcLpMkoopXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(gray_img, cmap='gray')\n",
        "plt.title('Original Input')\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(histeq, cmap='gray')\n",
        "plt.title('Histogram equalized')"
      ],
      "metadata": {
        "id": "BzA_LHZ0oopo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The  histogram equalization is that it's a nonlinear process. So, we cannot just separate out the three channels in an RGB image,\n",
        "equalize the histogram separately, and combine them later to form the output image.\n",
        "\n",
        "In order to handle the histogram equalization of color images, we need to convert it to a\n",
        "color space, where intensity is separated from the color information. YUV would be a good choice because the YUV model defines a color space in terms of one **Luminance (Y)** and two **Chrominance (UV)** components. Once we convert it to YUV, we\n",
        "just need to equalize the Y-channel and combine it with the other two channels to get the\n",
        "output image."
      ],
      "metadata": {
        "id": "g4UdcdNoo492"
      }
    }
  ]
}